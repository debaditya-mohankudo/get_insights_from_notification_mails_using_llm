(Auto-generated)


---

# ğŸ“˜ **README.md â€” GitHub Notification Email Insight Engine**

This project transforms your exported GitHub notification emails into a **searchable vector database**, enabling powerful natural-language queries about pull requests using a **local LLM** (Ollama).
It is fully offline, fast, and optimized for PR-specific retrieval.

---

# ğŸš€ Features

* Parse Apple Mail GitHub notification `.mbox` files
* Extract structured metadata:

  * PR numbers
  * Repository names
  * Jira tickets
  * Commit SHAs (normalized to first 7 chars)
  * Files modified
  * PR titles
  * Markdown sections (headings, lists, code blocks)
* Build a **FAISS HNSW** vector index
* Hybrid retrieval:

  * Weighted exact match
  * Semantic search fallback
  * Strict PR filter
* Query using a local model (`llama3.2:3b`)
* Returns commit summaries, file changes, PR description, etc.

---

# ğŸ§± Project Structure

```
.
â”œâ”€â”€ build_index.py           # Parse emails, extract metadata, build FAISS index
â”œâ”€â”€ query_llm.py             # Query engine + LLM orchestration
â”œâ”€â”€ email_models.py          # EmailMessage dataclass (commit truncation, full text)
â”œâ”€â”€ markdown_sections.py     # Markdown extraction utilities
â”œâ”€â”€ index.faiss              # Generated vector index
â”œâ”€â”€ meta.pkl                 # Serialized EmailMessage objects
â””â”€â”€ *.mbox/mbox              # Raw exported GitHub Apple Mail folders
```

---

# ğŸ“¥ 1. Preparing Your Data

Export your GitHub notification folders from **Apple Mail**:

1. Select the mailbox
2. Right click â†’ **Export Mailbox**
3. Place all exported folders in your project directory
4. Each exported folder contains a `mbox` file:

   ```
   repo_notifications.mbox/mbox
   ```

---

# ğŸ— 2. Building the Index

Run:

```bash
python build_index.py
```

What this script does:

âœ” Loads all `*.mbox/mbox` files
âœ” Parses each email body (plain + HTML â†’ cleaned)
âœ” Extracts:

* PR numbers (subject + Message-ID)
* Repos
* Tickets
* Clean PR title
* Commit list (regex match, normalized to 7 chars)
* Files modified
* Markdown sections
  âœ” Builds a combined text representation via `EmailMessage.full_text()`
  âœ” Encodes using **SentenceTransformers: all-MiniLM-L6-v2**
  âœ” Saves:

```
index.faiss
meta.pkl
```

---

# ğŸ” 3. Querying a PR

Use natural language:

```bash
python query_llm.py "pr #1234 commits and file changes"
```

The query engine:

### Step 1 â€” Extract PR number

Example: `"1234"`

### Step 2 â€” Weighted exact match

Scores PR numbers, repos, tickets, commits, file paths, and PR title.

### Step 3 â€” Strict PR filter

Ensures **only emails belonging to PR 1234** are considered.

### Step 4 â€” Semantic search fallback

Augments the query with all repo/PR/ticket tokens to improve vector recall.

### Step 5 â€” Format context chunks

Includes:

* commits
* files modified
* markdown code blocks
* headings
* lists
* first 1500 chars of email body

### Step 6 â€” Local LLM processing

Uses:

```python
ollama.generate(model="llama3.2:3b")
```

The LLM is instructed to:

* Answer **only about the exact PR**
* Use only retrieved email fragments
* Avoid hallucination

---

# ğŸ§  Example Output

```
[Exact-match retrieval â†’ 5 results]

==================================================
PR 1234 Summary:
- 5 commits
- 3 files modified
- Fixes on purchase history UI
- Replaced HTML download with JS PDF download
...
==================================================
```

---

# ğŸ§© Why This Works So Well

This project achieves high-precision PR answers because it uses:

### âœ” **Hybrid search**

Exact + semantic retrieval
â†’ almost never returns wrong PR emails.

### âœ” **Metadata-rich indexing**

Commits, PR titles, repos, tickets are treated as first-class search features.

### âœ” **Context shaping**

Context blocks contain structured + raw content.

### âœ” **Local LLM with clear rules**

Reduces hallucination significantly.

---

# ğŸ”§ Requirements

* Python 3.10+
* FAISS
* sentence-transformers
* BeautifulSoup4
* tqdm
* Ollama (for local LLM)

Install:

```bash
pip install faiss-cpu sentence-transformers beautifulsoup4 tqdm
```

You must also have:

```bash
brew install ollama
ollama pull llama3.2:3b
```

---

# ğŸ§ª Optional Improvements

I can help you implement:

* Change counting per file (`+/-` lines)
* Repo-level analytics
* Query augmentation via RAG-chain
* Web UI
* Embedding optimization (bge-small-en, nomic-embed-text, etc.)
* PR graph linking (threads/comments/commits)

---

# ğŸ‰ You're Done!

You now have a fully local, private, intelligent PR knowledge engine powered by GitHub emails.

---

If you'd like, I can also generate:

âœ… A **diagram** of the data flow
âœ… A **project architecture SVG**
âœ… A **flowchart**
âœ… A **demo GIF**
Just tell me which one.

